{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import CLIPTokenizer\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from diffusers import UNet2DModel, DDPMScheduler\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise Weights & Biases\n",
    "wandb.init(project=\"NatalieDiffusion\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset('NevskyCollective/nataliaXton')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the structure of the dataset\n",
    "print(dataset)\n",
    "print(dataset['train'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the tokenizer\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "class ImageCaptionDataset(Dataset):\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        image = item['image'].convert(\"RGB\")  # Ensure the image is in RGB format\n",
    "        caption = f\"label {item['label']}\"  # Adjust based on actual label usage\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        tokenized_caption = tokenizer(caption, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        return {\n",
    "            \"pixel_values\": image,\n",
    "            \"input_ids\": tokenized_caption[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": tokenized_caption[\"attention_mask\"].squeeze()\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "custom_dataset = ImageCaptionDataset(dataset['train'], transform=transform)\n",
    "\n",
    "# Create a DataLoader\n",
    "dataloader = DataLoader(custom_dataset, batch_size=8, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the UNet model and scheduler\n",
    "model = UNet2DModel.from_pretrained(\"google/ddpm-cifar10-32\")\n",
    "scheduler = DDPMScheduler.from_config(model.config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "try:\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            pixel_values = batch[\"pixel_values\"].to(device)\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            noise = torch.randn_like(pixel_values)\n",
    "            timesteps = torch.randint(0, scheduler.config.num_train_timesteps, (pixel_values.shape[0],)).to(device)\n",
    "            noisy_images = scheduler.add_noise(pixel_values, noise, timesteps)\n",
    "            outputs = model(noisy_images, timesteps)\n",
    "            loss = torch.nn.functional.mse_loss(outputs.sample, noise)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Log metrics to W&B\n",
    "            wandb.log({\"loss\": loss.item(), \"epoch\": epoch})\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "finally:\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model locally\n",
    "model.save_pretrained(\"./trained_model\")\n",
    "\n",
    "# Upload to Hugging Face\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()\n",
    "model.push_to_hub(\"your-username/your-model-name\")\n",
    "tokenizer.push_to_hub(\"your-username/your-model-name\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
